<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>)</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="icon" type="image/png" href="../../icon.png">
    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script>document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // Configuration optionnelle
            delimiters: [
                            {left: "$$", right: "$$", display: true},
                            {left: "$", right: "$", display: false}
                        ]
            });
        });
    </script>
</head>
<body>
<section id="vae-section">
<h1>Variationnal Auto Encoder (VAE)</h1>
<p>
In recent articles about image generation, the state-of-art seems to be accomplished by a model named <i>Variationnal Auto Encoder</i>.
To understand this model we need to go further in two keys concept : <i>Variationnal Inference</i> (VI) and <i>Auto Encoder</i> (AE).

<br><br>
</p>
</section>
<section id="vi-section">
<h2>Varitional Inference (VI)</h2>
<p>
Variationnal Inference is a branch of statistic which try to approach an unkonwn distribution from known distributions $\{q_{\theta}\}$. A way to approach the unknown distribution is for example by adjusting parameters of the knowned distrbution.

<br><br>
For example if you want to approximate a given distribution by using a Gaussian family. You have to set correctly $\mu$ and $\sigma$ until your gaussian distribution $q_{\mu,\sigma}$ is <i>similar</i> to the target distibution.
<img alt="gif" src="image/animation_pillow.gif"><br>

<br><br>
To start a <i>Variational Inference</i> it is necessary to get : 
1. A familly of parameterized known distributions $\{q_{\theta}\}$.
2. A metric to compare how far we are of the unkown distibution.

<br><br>
To explore these notions further, you can read <a>VI : a review for statisticians</a>.

<br><br>
</p>
</section>
<section id="vi-elbo">
<h3>General formulation of VI problem and ELBO</h3>
<p>
A natural metric to compare the divergence between two distibution $p$ and $q$ is the <b>Kullback-Leiber divergence</b> KL : 
$\textbf{KL(p||q)}=\int_Z q(z).\log{\frac{p(z)}{q(z)}}$ where Z is the definition set of p and q.

<br><br>
In pratice, we want to estimate a posterior distribution in order to solve machine learning problem.
Let $p(z|x)$ a posterior distribution and $p(x|z)$ the associates likelihood.

<br><br>
In order to estimate a posterior, we have data $x$. We set a statistic model and then get a model likelihood.

<br><br>
As a result, a <i>Variational Inference</i> (VI) problem with KL-divergence as a metric is formulated as :
$$ \argmin_{q\in Q} \textbf{KL}(q||p(z|x))$$
With expectation manipulations and Bayes rule the $\textbf{KL}$ become :

<br><br>
$\textbf{KL}(q||p(z|x))=-$<span style="border: 2px solid #d15a76; padding:3px">$(E[\log{q(z)}]-E[\log{p(x|z)}.p(z)])$</span>+$\log{p(x)}$

<br><br>
Maximise <span style="color:#d15a76">the red box</span> is equivalent to minimize the $\textbf{KL}$.
This quantity, in the red box, is the <b>ELBO</b> (Evidence Lower Bound).

<br><br>
In order to find the closest distribution $q$ of $p(z|x)$ we have to maximize the ELBO $(E[\log{q(z)}]-E[\log{p(x|z)}.p(z)])$

<br><br>
</p>
</section>
<section id="max-elbo">
<h3>How to maximize ELBO ? What are known quatities ?</h3>
<p>
We begin a VI problem with knwown data $x$. From this data, we build a statistic model with a prior distribution and a parametrized likelihood.
Then we set a form for $q$ like for example a Gaussian or another Exponential distribution.

<br><br>
We remember that we are looking for the posterior distribution. Since the distribution $p(x)$ is difficult to compute, we try to get the posterior by approaching it.

<br><br>
</p>
</section>
<section id="cav6">
<h3>CAVI : Coordinate Ascent VI</h3>
<p>
The basic algorithm to maximize ELBO and find the approximate distribution $q$ is <i>Coordinate Ascent VI</i> (CAVI) algorithm.

<br><br>
In order to compute at d-dimensions case, CAVI make an assumption on distribution. 
We suppose that each dimension are independent i.e : $q=\Pi_{i=1}^d q_j(z_j)$

<br><br>
The CAVI algorithm suggests to calculate the optimal distribution of direction $j$ only by fixing other variables along their direction.

<br><br>
It is possible to demonstrate that, under the <b>KL</b> as metric, the optimal distribution $q_{\theta}(z_j)$ (distribution which maximize ELBO) in for direction $j$ is proportionnal to $\exp(E_{-j}[p(z,x)])$. 
For mathematic details you can check the <i>Meerkat Statistic</i>'s course about VI.

<br><br>
Here is the algorithm :

<br><br>
Init : Set parameters randomly.
1. Calculate all optimal distributions $q(z_j)$. Note that the form of $q(z_j)$ depends of the form $q(z_i)$.
2. Compute the ELBO.

<br><br>
Then loop these steps until $|ELBO_t - ELBO_{t-1}| \lt \epsilon$.

<br><br>
</p>
</section>
<section id="gradient-ascent-vi">
<h3>Gradient Ascent VI and Normal Gradient</h3>
<p>
Note that, since the aim is to find the best $\theta$ by maximizing ELBO, it is sometimes possible to compute the gradient of the ELBO and to proceed at the optimization of $\theta$ similarly to a gradient descent. 

<br><br>
$$\theta^{t} = \theta^{t-1}+\alpha^t.\nabla\textbf{ELBO} $$

<br><br>
This method is called <b>Gradient Ascent VI</b>.

<br><br>
<u> <i>But is the gradient really a good metric to compare distributions ?</i></u>
The gradient (and derivative in general) are defined naturally from an euclidiean distance. 
Here it is an euclidiean distance in the space of the parameters.

<br><br>
Let's look at an example.
<img alt="" src="image/di-similar_gauss.png"><br>
Visually the first two distributions are similar, while the two others are barely overlapping.
However the canonic euclidean distance with the respect of $\mu$ says the inverse.

<br><br>
<b>The Euclidean gradient is sometimes not well adapted to VI.</b>

<br><br>
<u> The solution : Natural Gradient, a Riemanian gradient </u>
The solution, as explained in this <a href="https://arxiv.org/html/2406.01870v1">paper</a>, is to define a gradient in a Riemanian space with a symetric version of the <b>KL</b>.
This solution is also discussed in the <a href="https://arxiv.org/pdf/1206.7051">Stochastic VI paper</a>.

<br><br>
This gradient is named the <i>Natural Gradient</i> : $\nabla^{\text{natural}}=\mathcal{I}^{-1}].\nabla$.
It is the product of the inverse of the Fischer matrix and the original gradient.

<br><br>
As a result, we define the <i>Normal Gradient Ascent VI</i>, which uses the normal gradient in its formula.

<br><br>
Like for the <i>Gradient Ascent VI</i>, the <i>Normal Gradient</i> is simple to compute with an Exponential family.

<br><br>
Here a resume of VI method to get posterior distribution from the ELBO :
<img alt="" src="image/vi-methods/2.png"><br>

<br><br>
</p>
</section>
<section id="vi-limits">
<h3>Limitations of VI algorithm and Stochastic VI</h3>
<p>
The main issue with classic VI is that for each substep and iteration, we need to go through the entire dataset.

<br><br>
A natural improvement would be to use mini-batches, thus introducing stochastic behavior.

<br><br>
As a result, we adapt <i>Coordinate Ascent VI</i> (CAVI) into <i>Gradient Ascent VI</i> in its stochastic version, meaning the same algorithms, but using mini-batches instead of the full dataset.
<i>If you are not familiar with mini-batches methods you can check <a href="https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a">this link</a>.</i>

<br><br>
This opens the door to <i>Stochastic Varitionnal Inference</i>, with more scalable and better suited to large dataset algorithms.

<br><br>
</p>
</section>
<section id="autoenc">
<h2>Auto-Encoder</h2>
<p>

<br><br>
</p>
</section>
<section id="vae-section">
<h2>Variationnal Auto-Encoder (VAE)</h2>
<p>

<br><br>
</p>
</section>
<section id="img-app">
<h3>Application in image generation</h3>
<p>
</p>
</section>
<section id="nv-vae">
<h3>Nouveau Variationnal Auto-Encoder (NVAE)</h3>
<p>
</p>
</section>
</body>
</html>
